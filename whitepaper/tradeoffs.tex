% ====================================================================
\chapter[Conclusions, Tensions and Trade-offs]{Conclusions, Tensions and Trade-offs}
\def\chpname{tradeoffs}\label{chp:\chpname}

Chapter editors:
\credit{ivezic},
\credit{StephenRidgway},
\credit{drphilmarshall}


In this chapter we summarize the findings of the science cases in this
white paper, and propose some actionable conclusions for the LSST Project.
We then discuss the tensions and tradeoffs apparent in these findings.

% ---------------------------------------------------------------------

\section{Summary of Cadence Constraints}

\credit{ivezic}

The authors of the preceding chapters' science cases provided guidelines
for improving the baseline LSST cadence, via their 10-question
conclusions sections.  We summarize this input below, extracting a number
of recommendations for the Project to consider. The most important suggested action
items for the Project Team include i) implementation, analysis and
optimization of the ``rolling cadence'' idea, and ii) execution of a
{\it systematic} effort to further optimize the ultimate LSST cadence
strategy.

\begin{description}

\item[Q1:] {\it Increased sky coverage is possible, at the price of fewer visits per field and shallower
coadded depth.  Does the science case place any constraints on the
tradeoff between the sky coverage and depth? For example, should
the sky coverage be maximized (to $\sim$30,000 deg$^2$, as e.g., in
Pan-STARRS) or the number of detected galaxies (the current baseline but
with 18,000 deg$^2$)?}

A general conclusion from many science cases is that the sky coverage
can be increased only if the baseline single-visit and coadded depths
are not significantly degraded. For example, the number of galaxies above
some threshold photo-z quality would increase by about 8\% with the larger
sky area, but the redshift range would be diminished a bit. Large-scale
structure studies (\eg galaxy clustering, BAO) are
area-limited more than depth-limited and thus a larger sky coverage is
preferred for them. In time-domain cases where the LSST baseline performance is already
more than adequate, a larger sky coverage is preferred, too (AGNs,
extra-galactic cepheids); when cadence performance is barely sufficient
or insufficient, smaller sky area with better temporal sampling is suggested
(supernovae, strong lensing).

The Project would enable finer optimization by providing several more
simulations with the sky coverage in between the baseline cadence sky
area of 18,000 deg$^2$) and the so-called ``Pan-STARRS cadence'' with
$\sim$30,000 deg$^2$. MAF analysis should be extended to compute the
effective number of galaxies good for weak lensing, as well as to track
the performance of star-galaxy separation.


\item[Q2:] {\it Does the science case place any constraints on the
tradeoff between uniformity of sampling and frequency of  sampling? For
example, a rolling cadence can provide enhanced sample rates over a part
of the survey or the entire survey for a designated time at the cost of
reduced sample rate the rest of the time (while maintaining the nominal
total visit counts).}

Supernovae provide one of the strongest drivers for the implementation of
rolling cadence. A sampling rate about three times higher than the
uniform sampling implemented in the baseline cadence (revisit time scale of
about one day), and lasting 3-4 months, is suggested. It is likely that
such a rolling cadence would also be beneficial for constraining
asteroid orbits and for studying short time scale variables (e.g.
cataclysmic variables). Extreme cases of a rolling cadence, with roughly
week-long campaigns, for special sky regions would be beneficial for
studies of Young Stellar Objects.

Other types of transients also require a rolling cadence, with necessary compromises.  A faster cadence in
bluer filters in a shorter rolling window, and slower cadence in redder filters, may help address the
trade-offs.

The production and analysis of several families of rolling cadence
simulations should be a high priority, because this
baseline cadence modification might provide more significant science
benefits than any other proposed modification.


\item[Q3:] {\it Does the science case place any constraints on the
tradeoff between the single-visit depth and the number of visits
(especially in the $u$-band where longer exposures would minimize the
impact of the readout noise)?}


A large number of science cases would benefit from deeper $u$ band data
(both single-visit and co-added depth), as long as the number of visits
is not decreased (of course, increasing the number of $u$ band visits
would be beneficial, too).

Simulated cadence \opsimdbref{db:DoubleUbandExptimeSameVisits}, which
doubled the $u$ band exposures from 30 sec to 60 sec while retaining the Baseline number of visit, demonstrated that an
improvement of $u$ band single-visit depth of 0.6 mag can be achieved
with only a minor loss of coadded depth and the number of visits in other bands.
We advocate taking u band exposures that are long enough to be sky noise dominated.
The Project should investigate this option further and potentially
modify the baseline per-band exposure time allocation.


\item[Q4:] {\it Does the science case place any constraints on the
Galactic plane coverage (spatial coverage, temporal sampling, visits per
band)?}

A general comment made by science cases that depend on the Galactic
plane coverage is that Figures of Merit (FoM) exist, but that ``many
OpSim runs with a variety of temporal distributions of exposures within
the ten-year survey lifetime [are needed] to make these FOMs useful.''

The Project could consider producing a family of simulations that explore
different cadence strategies for the Galactic disk coverage.


\item[Q5:] {\it Does the science case place any constraints on the
fraction of observing time allocated to each band?}

Generally, the baseline time allocation is satisfactory. Potential
improvements depend on studied population (e.g. young stellar objects)
and the required imaging depth.
The case for deeper $u$ band data (both single-visit and coadded depth)
has the strongest drivers. The fraction of visits in red bands should be
higher for deep drilling fields than for the main survey.

When producing a new generation of deep drilling simulations, the
Project should revisit the per-band exposure time allocation for deep
drilling fields.


\item[Q6:] {\it Does the science case place any constraints on the
cadence for deep drilling fields?}

The strongest constraints for the deep drilling field (DDF) cadence come
from supernovae. DDFs should be preferrably visited nightly,
1-2 mag deeper than for the main survey visits. At least a few
extragalactic fields should be observed at any month, with
each field observed for a ``season'' of length at least 120-150 days,
staggering the fields so new fields cycle in as old ones cycle out, and
avoiding the moon. For
potential deep drilling fields within the inner Galactic plane, short
temporal baselines would be very useful. In the case of Young Stellar
Populations, a single week of denser monitoring for specific regions is
suggested, with the goal of having several observations per night,
separated in time by at least an hour from one another.


\item[Q7:] {\it Assuming two visits per night, would the science case
benefit if they are obtained in the same band or not?}

Most science cases prefer pairs of visits in two different bands.
Exceptions are GRBs and asteroids; in the case of GRBs the second visit is
needed to promptly establish variability and in the case of asteroids
different bands result in decreased sample completeness.

The Project could investigate whether it is possible to increase the
fraction of visit pairs with different bandpasses.


\item[Q8:] {\it Will the case science benefit from a special cadence
prescription during commissioning or early in the survey, such as:
acquiring a full 10-year count of visits for a small area (either in all
the bands or in a  selected set); a greatly enhanced cadence for a small
area?}

A lot of excellent suggestions were made by multiple science cases.
For example, photo-z analysis would be greatly assisted by acquiring
a full 10-yr count of visits for a small area during commissioning or
early in the survey, especially if these regions would be also covered by
existing spectroscopic surveys.
The Comissioning Science Verification Team should incorporate these
suggestions, when possible, into the Commissioning plan, as well
as to socialize the existing plan for ``mini surveys'' with Science
Collaborations.


\item[Q9:] {\it Does the science case place any constraints on the
sampling of observing conditions (e.g., seeing, dark sky, airmass),
possibly as a function of band, etc.?}

The strongest drivers are formulated for obtaining good seeing data (the
weak lensing case for good seeing in the $r$ and $i$ bands is already
implemented in the simulator) and for minimizing various correlations
(e.g. parallax factor vs. hour angle).

The Project could analyze the impact of secular effects on randomizing
various correlations vs. an explicit algorithmic driver to do so
implemented in the Scheduler.


\item[Q10:] {\it Does the case have science drivers that would require
real-time exposure time optimization to obtain nearly constant
single-visit limiting depth?}

No strong driver for real-time exposure time optimization was
identified.

The Project could monitor the fate of a similar proposal considered by
the upcoming Zwicky Transient Facility.

\end{description}


% --------------------------------------------------------------------

\section{Tensions and Tradeoffs}

\credit{StephenRidgway}

The LSST survey will be carried out with physical and operational
constraints that will impact all science objectives.  These include
design limitations, such as the aperture of the telescope, the detector
noise and readout time, and the limited number of filter changes that
can be supported during the lifetime of the survey.  They include
natural constraints, such as the quantity of useful observing time
in 10 years, and system optimization constraints, such as
exposure time and sky coverage.

The LSST observing schedule can be designed, to some extent, to minimize
the impact that these limitations have on any one or few science
objectives. As the science objectives become more numerous and more
complex, the optimization becomes more difficult and the chances
increase that significant compromises may be required.

In the science chapters of this report, science objectives are
described, and for each, diagnostic metrics, and in some cases figures
of merit, are designed to represent quantitatively the interests of that
topic in a schedule optimization.  Not all of these metric sets are
fully worked out, and in most cases they are provisional pending further
analysis and community review and input.  However, they do already
suffice to bring attention to many special requirements.

The design of the LSST scheduler, and of the algorithms that will select
the visit sequences, has a considerable distance to go before hard
cadence questions must be confronted and resolved.  However, it is
already possible to survey the reach of science needs, and to identify
areas of competition which may become candidates for careful trades and
decisions in the years before the survey begins.

In the rest of this chapter, we review the possible tensions that are
now evident, and where tradeoffs may become necessary. Most potential
tensions within the main survey concern {\it temporal sampling for
variable targets}.  Tensions among static science objectives, and
between static science and variable science, may be less likely and
mild. The strongest points of tension {\it may} turn out to be between
mini-surveys and the main survey.

% --------------------------------------------------------------------

\subsection{Discussion: Variable Targets -- Where's The Tension?}

Strictly periodic targets are relatively neutral to cadence speed, since
successive periods can be combined to improve phase coverage steadily
through the survey.  The only odd cases are ultra-short ($<~ $1 minute)
and ultra long ($> $10 years) periods, and periods very, very close to
one sidereal day.  However, with precision measurements over a long
term, some of the very interesting results for periodic variables will
be in period drifts or slight deviations from periodicity. Furthermore,
even periodic targets benefit from an early interval of higher frequency
sampling, at least in some sky regions, as this can accelerate the ramp
up of the science.

By far the majority of variables and transients, stellar and galactic,
are not periodic. For these, study will be greatly simplified (or may
absolutely require) sufficient sampling within an interval that depends
on the target type. One would like to satisfy the sampling theorem, with
visits at twice the frequency of the highest frequency content, but this
is only a conceptual guide - knowledge of, and experience with, the
targets and the science objectives can provide practical criteria.

A truly uniform cadence provides a revisit rate of one visit pair every
16 days (in the $r$ or $i$ bands), or every 3.7 days (in any filter) --
assuming a 5 month observing season.  This is a sparse sample rate for
many variable types.  Achieving higher sample rates requires (possibly
very strong) deviations from complete uniformity.  Thus the obvious
conclusion: rapid cadences cannot apply everywhere all the time. Rapid
cadences must be designed, executed selectively, bounded by the number
of visits available, and coordinated with all other such cadences, as
well as more general survey requirements.

\subsubsection{Examples}

Several examples will illustrate the diversity of cadences that are
represented in the science programs described in this white paper.

QSO variability tends to be stronger in low temporal frequencies   A uniform distribution of the
LSST visits, with minimal seasonal gaps, provides fairly good support
for identifying QSOs from their variability pattern.

For a SN, sufficient sampling must be acquired during the life of the
event. A good cadence in at least one filter is required to support
classification, and multiple colors to support photometric redshift
determination.  A uniform LSST cadence, even with large seasonal gaps,
does not provide a sufficient sampling rate for SN science - an
enhancement of a factor of 2 or greater is strongly requested.

To determine the rotational period of stars with spots, sampling must
resolve light variations sufficiently to constrain periodicity {\it
within} the spot lifetime, which is typically several weeks. This
cadence is much more rapid than provided by a uniformly distributed WFD
visit pattern.

Flaring stars and interacting binaries may show dramatic flux changes
in minutes to hours, and correct identification of such events may
require several data points, and possibly more than one filter, on a
similar time scale.

The solar system small body case is particularly complex.  The science
is one of the main LSST drivers.  Detection of PHAs has a non-scientific
and even political component. Asteroid confusion can interfere with
transient discovery. The density of targets is a strong function of
position on the sky.  Characterization of solar system objects, by
determination of orbits, requires visit patterns on short timescale
($\approx$ hour return) and intermediate time scale ($\approx$2 weeks) -
long timescale confirmation occurs naturally later in the survey.  The
number and pattern of rapid revisits required for positive
identification depends strongly on the false positive rate.


\subsubsection{How to provide a range of cadence speeds}

The problem of sampling diverse events was of course recognized very
early in survey planning. Previous cadence development has explored the
following special cadence options:

\begin{description}

\item{Rapid revisits} -- this feature was introduced for study of solar
system bodies and most schedule simulations give high priority to
acquiring visits in pairs with $~$30 minute separation.  Experiments
have been done with triples, in case that should prove necessary for
asteroid characterization.  The possibility of using a different revisit
pattern in different parts of the sky (e.g. less frequent away from the
ecliptic) has been mentioned.  Different patterns for different filters
(e.g. not using pairs for visits in the $u$ and $y$ bands) have been
suggested and investigated. The use of visit pairs is clearly a very
large impact decision, for practical purposes reducing by a factor of 2
the effective revisit rate for other targets. However, rapid pairs are
very effective for measuring brightness gradients for rapidly varying
objects, and thus particularly valuable for the difficult problem of
characterizing blank sky transients.

\item{Mini-surveys} --  these can include special cadences. The deep
drilling concept utilizes rapid visit sequences to achieve greater depth
without saturation of detector wells, giving sky-limited true time
series with $\approx$30 second sampling steps.  The possibilities for
mini-surveys are limitless, but of course they are bounded by the amount
of time available outside the main survey. The trade between
mini-surveys and the main survey is discussed below.

\item{Rolling cadence} -- this would allow for the possibility of
re-deploying visits within the main survey so as to respond to special
cadence demands without compromising main survey goals (or, perhaps in
principle, trading against main survey goals in a measured and optimum
way). Rolling cadences were introduced, and some preliminary experiments discussed, in \autoref{sec:rolling}.  As an example, the
average 9 visit pairs per year in the $r$ filter, which would be
distributed over a season in a uniform survey, could be distributed over
2 months, 1 month, 1 week, or 1 day, in a rolling cadence (leaving no
visits in $r$ for the rest of the season).  Or, more conservatively for
the main survey, half (4-5 visit pairs) could be spent in an enhanced
visit rate, reserving the other half to maintain visit pairs in the rest
of the season.  Also, a rolling cadence can concern any number of
filters; for example, one filter could be used to provide short bursts
of rapid sampling, while other filters could maintain a uniform
distribution.  Different rolling cadences can be used in different parts
of the sky, or at different times during the survey.  There is an
immense range of possibilities for rolling cadence, and the surface has
barely been scratched.

\item{Commissioning survey} - the highest priority of the commissioning
schedule is, of course, commissioning.  However, a secondary objective
is to demonstrate system operation in the planned survey mode -
presumably including main survey, deep drilling, rolling cadences, etc.
There have been other suggestions, going beyond these basics, such as
integrating some fields to the full survey depth, or acquiring some
special cadences.  However, there is no assurance that all of these will
be possible, and they will be prioritized following formal commissioning
objectives.

\end{description}

\subsubsection{Other options for special cadences}

\begin{description}

\item{Pre-survey options} -- there are a number of survey instruments
(CTIO, CFHT, Subaru) that can easily reach the single visit depth of
LSST. These resources could be used to explore limited sky regions
($\approx$1\% of the LSST sky) with cadences that are planned for LSST
(or cadences that are not planned for LSST), providing touchstone
datasets especially for the more common target types that will dominate
the survey.

\item{Twilight survey} -- \autoref{sec:shortexp} describes a concept for
twilight data acquisition, using short exposures to tolerate bright sky.
This time is not required for current LSST science, and in principle
could be allocated to $z$, $y$ filters in short bursts (20 minutes) of
fast cadence ($<~$15 seconds) imaging, within the sensitivity limits of
the twilight sky.

\item{Follow-up} -- LSST is, in large part, a discovery machine. It is
not realistic to expect LSST to provide its own follow-up for {\it all}
possible target types and characteristics. Fortunately, many of the most
useful discoveries will be bright enough to follow-up with smaller, more
accessible apertures.  Follow-up can be far more customized to the
science needs than a general purpose LSST cadence.  Faint targets of
sufficient value may likewise merit followup with exceptional  ground
and space-based resources.

\end{description}


\subsubsection{Frequency of filter changes}

Multi-color visits are a very special case for LSST.  Moving the large
(huge) optical filters involves substantial structure and mechanisms.
While filter change time is not fully characterized, it will be slow
enough (about 2 minutes) that filter change frequency competes directly
with efficiency. Also, the mechanisms have a finite lifetime - ``rapid''
multi-color sequences will be the exception rather than the rule.

Many science objectives for variable targets are best served by
simultaneous or nearly so color information.   It is important to identify
when and if rapid filter changes are of value or essential, and to trade this
against the efficiency and limited lifetime of the filter change mechanism.


\subsubsection{Tension between rapid and slow cadences}

In summary, we can readily identify competing demands for very different
cadences, including fast cadences in multiple ranges. For characteristic
times ranging from  $\lesssim$1 minute to $\sim$1 month, a uniform visit
distribution cannot be fully satisfactory, and in some cases it may turn
out to be totally unsatisfactory.  A number of concepts for alternate
cadences are available.  None can provide rapid cadence all the time
over all the sky. It may be possible to provide cadences matched to most
requirements over part of the sky all of the time, and over all of the
sky at some time. For transient targets, a complex survey cadence may
obtain limited duration but ``appropriate'' sampling of a fraction of
the actual events, with the exact fraction still to be decided but
inevitably significantly less than one.

The tension in scheduling is between science objectives and limited
scheduling flexibility. The confrontation between science requirements
and schedule performance leans on the metrics and merit functions that
are the major goal of this white paper.  It should be clear from the
foregoing chapters that the difficult goal of metrics analysis is not in
describing sampling for the science, which is ``easy.''  The more
difficult part is in determining the number of science targets for which
adequate sampling can be provided by a simulation, and perhaps the
greatest challenge is determining how many such targets with the
specified sampling are required for a science objective.  It is only
when this step has been accomplished for a large part of the science
that competition between the science objectives can become a
quantitative process.

% --------------------------------------------------------------------

\subsubsection{Discussion: Static Target Science -- Is There Any Tension?}

The needs of static target science appear to have fewer points of
potential tension among them than variable targets.  The major
cadence-related concerns are:

\begin{description}

\item{Photometry} -- the best photometric performance will be achieved
after the calibration has been closed around the sky with superior image
quality and superior photometric quality visits to every field.  While
this is probable due to randomization of conditions over 10 years, the
sooner that it is achieved during the survey, the sooner high quality
photometry will be available.  This could be a target of active schedule
control, with corresponding decreasing flexibility in some other
schedule variables.

\item{Astrometry} -- both proper motions and parallaxes are served by
any schedule that spreads visits well over the duration of the survey.
Parallaxes benefit from observing at a range of hour angles (for control of HA-dependent biases),
which is slightly in competition with the preference to observe at small airmass
for best image quality, but typical simulated schedules show good
astrometric performance. A rolling cadence that moved a significant
fraction of visits from one time period of the survey to another could
impact the astrometric performance (either for better or for worse),
though as long as the fraction of visits concerned was small, the effect
would correspondingly be small.

\item{Homogeneity} -- an  example of required homogeneity is image
quality. Just as the atmosphere offers a range of image quality during a
night and from night to night, each point on the sky will be observed
with a range of image quality.  To enable understanding of selection
effects, and to compare sky regions on an even playing field, it is
desirable that for each filter, all parts of the sky should be observed
with a similar distribution of image quality, and in particular with
similar best image quality. Achieving homogeneity of conditions actively
could be quite challenging, but simulations show that with a large
number of independent visits, it occurs naturally to good approximation.
Any cadence that relied on concentrated bursts of visits in a short
interval would tend to reduce the spread of conditions observed.
However, such an extreme has not been proposed or studied.

\item{Randomization} -- closely related to homogeneity, randomization is
means of achieving homogeneity in some observing parameters.  Examples
are the projected angles of camera and telescope optics on the sky.
These are less random than sky conditions, as they depend on instrument
setup and schedule history.  Simulations show that optics angles are
well randomized passively (i.e. without scheduler optimization) for most
points on the sky, but not for all.  Randomization could be improved,
for example by actively running the camera rotator when advancing from
one sky position to the next, in order to populate under represented
camera angles. The rotation takes time, and could reduce the overall
efficiency of the survey.  Only simulations can explore the impact of
these additional mechanical motions.

\item{Dithering} -- dithering of visits is a powerful method of
improving homogeneity of sky coverage passively. Few compromises have
been identified with dithering thus far.  Dithering for small regions
has a price. Imagine the loss in depth due to large dithers with a
single FOV, e.g. a deep drilling field (this has not been proposed).
Due to this effect, certain rolling cadences can have a potential small
loss of efficiency or efficacity when implemented with dithering.

\end{description}

The foregoing shows that within the static science domain, there are few
and mild points of tension and potential competition.

% --------------------------------------------------------------------

\subsection{Discussion: Tension Between Static And Variable Science}

For the most part, the tensions between static science and variable
science are modest and easily understood.  A variable-driven cadence
that requires special timing of visits may result in loss of efficiency
due to increased slew times, or observing under less optimum conditions
(larger airmass).  Special cadences are likely to reduce randomization
and homogenization to a small degree. However, except for very
aggressive cadence implementations, these are second-order effects.
Furthermore, they are measurable with simple metrics - the impact of
variable science schedule considerations on static science should be
small, and it can be readily quantified.

% --------------------------------------------------------------------

\subsection{Discussion: Mini-surveys and the Main Survey -- Tension for Sure}

The LSST proposal and current plan allow a fraction of the total survey
time for mini-survey. These may cover either special sky regions,
special cadences, or both. The fraction 10\% has been carried for
mini-surveys, but this is not necessarily a sacred number. In
\autoref{chp:cadexp}, current scheduling experience has shown that the
main survey program, to design depth, can be accomplished in $\approx$
85\% of the available time. However, improvements in simulations could
move this estimate up or down. Adequacy of the design depths could be
reconsidered.  And of course the execution of the survey could encounter
unprecedented circumstances.

Proposals for mini-surveys include deep drilling fields, the northern
ecliptic, and the Magellanic clouds. Notional suggestions for deep
drilling fields alone would exceed 10\%.   Most schedule simulations
have allocated a limited number of visits to the Galactic plane (based
on the expectation that crowding would limit the useful stacked depth).
However, as detailed in \autoref{chp:galaxy}, many areas of Galactic
science could benefit from a more aggressive visit plan, perhaps similar
to the main survey.

At present there is no evidence that the trade between main and
mini-surveys will require difficult compromises.  But it is a natural
area of tension, and since is subject to weather experience (not to
mention the evolution of the science) it is likely to be with us through
the life of the survey.

% --------------------------------------------------------------------

\subsection{Final Thoughts}

The likely points of technical and scientific tension in scheduling are
apparent from the schedule simulation experience (\autoref{chp:cadexp})
and the science objectives and metrics
(\autoref{chp:solarsystem}--\autoref{chp:specialsurveys}).  Static
science has relatively few and mild points of concern.  Variable science
has little and moderate tension with static science.  Variable science
has many points of tension between different variable science
objectives, owing to the wide range of time scales. These lead to
contrasting technical demands, and may or may not prove to be areas of
scientific competition.

The essential information needed to clarify tensions is the
determination, for each science objective, of three things.
\begin{itemize}
	\item The number of such targets required for the science;
	\item For a simulated schedule, the number of instances of targets satisfactorily observed;
	\item The cadence requirements.
\end{itemize}
This information is the key to calibrating the metrics in terms of
absolute and relative  sufficiency.

The science teams are urged to continue to define and refine metrics, and the MAF developers to provide
tools for comparing simulated schedules by means of metrics.

% ====================================================================
